{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db =SQLDatabase.from_uri(\"mysql://root:123456@localhost:3306/test\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True,\n",
    "                                  agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as check that for langchain sql isn't work well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1045 (28000): Access denied for user 'root'@'172.17.0.1' (using password: YES)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mselect * from user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m df \u001b[39m=\u001b[39m execute_mysql_query(query\u001b[39m=\u001b[39mquery)\n\u001b[1;32m---> 41\u001b[0m df\u001b[39m.\u001b[39;49mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Replace the following values with your actual database credentials\n",
    "database_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"1234\",\n",
    "    \"database\": \"test\",\n",
    "}\n",
    "\n",
    "def execute_mysql_query(query):\n",
    "    try:\n",
    "        # Connect to the MySQL server\n",
    "        connection = mysql.connector.connect(**database_config)\n",
    "\n",
    "        if connection.is_connected():\n",
    "            # # Get the MySQL server version\n",
    "            # cursor = connection.cursor()\n",
    "            # cursor.execute(\"SELECT VERSION()\")\n",
    "            # data = cursor.fetchone()\n",
    "            # print(\"Connected to MySQL Server Version:\", data[0])\n",
    "\n",
    "            # cursor.execute(\"SELECT * FROM user\")\n",
    "            # rows = cursor.fetchall()\n",
    "            # for row in rows:\n",
    "            #     print(row)\n",
    "            df = pd.read_sql(query, con=connection)\n",
    "            return df\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Error:\", err)\n",
    "\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection.is_connected():\n",
    "            # Close the connection if it's open\n",
    "            connection.close()            \n",
    "            \n",
    "query = \"select * from user\"\n",
    "df = execute_mysql_query(query=query)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandasai import PandasAI\n",
    "from pandasai.llm.openai import OpenAI\n",
    "\n",
    "# api_key =\"\"\n",
    "\n",
    "llm = OpenAI(api_token=api_key)\n",
    "pandas_ai = PandasAI(llm)\n",
    "\n",
    "pandas_ai(df, prompt='Who is smaller?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.\"\"\"\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.write(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "raw_docs = TextLoader('test.txt').load()\n",
    "text_spliter = CharacterTextSplitter()\n",
    "docs =text_spliter.split_documents(raw_docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is text embedding model\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.\n"
     ]
    }
   ],
   "source": [
    "em_ve = embedding.embed_query(query)\n",
    "print(db.similarity_search_by_vector(em_ve)[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker redis\n",
    "\n",
    "``sh\n",
    "docker run --name rds -dp 6379:6379 redis/redis-stack-server \n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.redis import Redis\n",
    "\n",
    "rds_url = \"redis://localhost:6379\"\n",
    "index_name = \"link\"\n",
    "\n",
    "rds = Redis.from_documents(docs, embedding, redis_url=rds_url, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(query)[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.', metadata={'source': 'test.txt'}), Document(page_content='Ankush went to Princeton', metadata={}), Document(page_content='Ankush went to Princeton', metadata={}), Document(page_content=\"If you're not interested in the keys of your entries you can also create your redis instance from the document\", metadata={})]\n"
     ]
    }
   ],
   "source": [
    "retriver = rds.as_retriever()\n",
    "\n",
    "docs = retriver.get_relevant_documents(query)\n",
    "print(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try with transformers embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b29b8adb3a424d984a719787528779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2885fbfd8c741488373438f24a99d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928a6e75d70c452c9d587140a3c8339d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9169596f03fb41a09c48b81d47df47d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52e5454f72d486095e8bfc9fd9187f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868d1b7ed3c241be9ab4e7e5a30ac751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed78a8cb76eb4fd381d1c009473aba68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31836cb05ae4d68954ef693b1ee2cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d892507d14da448dba9bda2cdd71131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b54ec9d3a34b009745975c662f6e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d176c8818a428d93501eb1ecfe76d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1398dca064ba4b378b1dc7280f8642f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f19fed61f4d4f40a534c764c4684c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004c775b186f4473ae216c96364f953f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "model_id = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "query_res = embedding.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "doc_result = embedding.embed_documents([text, \"This is not a test document.\"])\n",
    "print(len(doc_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter' metadata={'start_index': 0}\n",
      "page_content='splitter takes a list of characters. It tries to create chunks based on splitting on the first' metadata={'start_index': 87}\n",
      "page_content='the first character, but if any chunks are too large it then moves onto the next character, and so' metadata={'start_index': 172}\n",
      "page_content='and so forth. By default the characters it tries to split on are [\"' metadata={'start_index': 264}\n",
      "page_content='\", \"\\n\", \" \", \"\"]' metadata={'start_index': 333}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_spliter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10, length_function=len, add_start_index=True)\n",
    "\n",
    "text = \"\"\"The default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]\"\"\"\n",
    "texts = r_spliter.create_documents([text])\n",
    "for t in texts:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.predict(\"could you please provide me a sample contract text?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_index\n"
     ]
    }
   ],
   "source": [
    "# create embeddings\n",
    "docs_new = r_spliter.create_documents([res])\n",
    "\n",
    "rds = Redis.from_documents(docs_new, embedding=embedding, redis_url=rds_url, index_name='new_index')\n",
    "\n",
    "print(rds.index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Furniture Rentals Contract\n",
      "\n",
      "This agreement is made and entered into this day of____, 20__, between [Company A] (“Company A”) and [Company B] (“Company B”).\n",
      "\n",
      "This Furniture Rentals Contract (the “Agreement”) sets forth the terms and conditions under which Company A will provide furniture rental services to Company B.\n",
      "\n",
      "1. Furniture Rentals. Company A agrees to provide furniture rental services to Company B as specified by Company B in a separate written order form and/or invoice issued by Company A.\n",
      "\n",
      "2. Terms of Payment. Company B agrees to pay for the furniture rental services provided in accordance with the payment terms set forth in the separate order form and/or invoice.\n",
      "\n",
      "3. Term. This Agreement shall begin on the Effective Date and shall continue for a period of one year, unless terminated earlier in accordance with Section 4.\n",
      "\n",
      "4. Termination. This Agreement may be terminated by either party upon thirty (30) days written notice to the other party.\n",
      "\n",
      "5. Governing Law. This Agreement shall be governed by the laws of the [State], without giving effect to any principles of conflicts of law\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Furniture Rentals Contract\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Furniture Rentals?\"\n",
    "\n",
    "query_embedding = embedding.embed_query(query)\n",
    "\n",
    "out = rds.similarity_search(query)\n",
    "print(out[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_text = r_spliter.split_text(res)\n",
    "\n",
    "r = embedding.embed_documents(splited_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Furniture Rentals Contract' metadata={'start_index': 2}\n",
      "page_content='Furniture Rentals Contract' metadata={'start_index': 2}\n",
      "page_content='which Company A will provide furniture rental services to Company B.' metadata={'start_index': 251}\n",
      "page_content='which Company A will provide furniture rental services to Company B.' metadata={'start_index': 251}\n"
     ]
    }
   ],
   "source": [
    "for x in out:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hands-on with real contract\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Redis\n",
    "\n",
    "\n",
    "model_id = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_id)\n",
    "# text loaer\n",
    "loader = TextLoader('sample_contract/chat_code.txt')\n",
    "texts = loader.load()\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=100, \n",
    "                                              chunk_overlap=20,  \n",
    "                                              length_function=len, \n",
    "                                              add_start_index=True)\n",
    "\n",
    "docs = text_spliter.split_documents(texts)\n",
    "\n",
    "# create redis vectorstore\n",
    "rds_url = \"redis://localhost:6379\"\n",
    "index_name = \"link_new\"\n",
    "\n",
    "rds = Redis.from_documents(docs, embedding, redis_url=rds_url, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc:link_new:7552e4c349b4477dbc1693d3bdff52da']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.add_texts(['Here we go over different options for using the vector store as a retriever.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Furniture Rentals. Company A agrees to provide furniture rental services to Company B as', metadata={'source': 'sample_contract/chat_code.txt', 'start_index': 319}),\n",
       " Document(page_content='A] (“Company A”) and [Company B] (“Company B”).', metadata={'source': 'sample_contract/chat_code.txt', 'start_index': 108}),\n",
       " Document(page_content='to Company B as specified by Company B in a separate written order form and/or invoice issued by', metadata={'source': 'sample_contract/chat_code.txt', 'start_index': 395}),\n",
       " Document(page_content='provided in accordance with the payment terms set forth in the separate order form and/or invoice.', metadata={'source': 'sample_contract/chat_code.txt', 'start_index': 583})]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rds.similarity_search(query='different options ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain with pipeline for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModel, AutoModelForDocumentQuestionAnswering\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_prompt = \"\"\"Scenario: You are a legal professional working for a law firm that deals with various types of contracts. Your firm has recently developed an advanced content search engine that utilizes AI language model, LLM, to assist in analyzing and extracting relevant information from legal contracts.\n",
    "\n",
    "Content: You start by providing LLM with a snippet of a legal contract. The contract excerpt contains specific clauses related to confidentiality, termination, and dispute resolution. Describe these clauses in detail, including any key terms or conditions mentioned.\n",
    "\n",
    "Query: After providing the content snippet, you ask LLM the following question:\n",
    "\n",
    "\"Based on the provided contract excerpt, can you identify any potential issues or risks related to confidentiality, termination, or dispute resolution? Additionally, please suggest any best practices or recommendations for handling these clauses effectively.\"\n",
    "\n",
    "This prompt sets the stage for a conversation where LLM can analyze the provided content (the contract snippet) and extract relevant information related to confidentiality, termination, and dispute resolution. LLM's response can include identifying potential legal risks or concerns in those clauses, along with offering best practices and recommendations for handling these critical contract elements effectively. This interaction demonstrates how the content search engine powered by LLM can be a valuable tool for legal professionals to analyze and understand complex legal contracts.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MLoong\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nThe approach to analyzing your legal contract is', generation_info=None)]], llm_output=None, run=[RunInfo(run_id=UUID('c56c6a23-3952-45f4-b2e1-962f95715a1d'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.generate(prompts=[openai_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
